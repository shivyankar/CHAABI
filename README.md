# CHAABI
Assignment by CHAABI

Why I used model as burt-based-uncased and deepset/roberta-base-squad2

Vector Embeddings:
Model: bert-base-uncased
Task: Feature extraction for creating vector embeddings.
Purpose: This model is commonly used for general-purpose language understanding and is suitable for creating vector representations of text. It's a transformer-based model that can capture contextual information in the input text.

Language Model for Question Answering:
Model: deepset/roberta-base-squad2
Task: Question answering using the SQuAD 2.0 dataset.
Purpose: This model is fine-tuned specifically for the task of question answering. It has been trained on the Stanford Question Answering Dataset (SQuAD) and is designed to extract answers from a given context. It's based on the RoBERTa architecture, which is a variant of BERT.
In summary, bert-base-uncased is used for creating vector embeddings, and deepset/roberta-base-squad2 is used for answering contextual questions based on the embeddings generated by the first model. Each model is chosen based on its suitability for the specific task it is intended to perform.
